{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid,methode='RandomForestRegressor'):\n",
    "    '''return mean absolute error'''\n",
    "    if methode == 'RandomForestRegressor':\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_valid)\n",
    "        return mean_absolute_error(y_valid, preds)\n",
    "    else:\n",
    "        return 'working on it, please wait'\n",
    "\n",
    "def impute(X_train:pd.DataFrame,X_test:pd.DataFrame)->tuple:\n",
    "    '''\n",
    "    填充缺失值 如 np.nan\n",
    "\n",
    "    Univariate imputer for completing missing values with simple strategies.\n",
    "\n",
    "    Replace missing values using a descriptive statistic (e.g. mean, median, or most frequent) along each column, or using a constant value.\n",
    "    '''\n",
    "    # Imputation\n",
    "    my_imputer = SimpleImputer() # SimpleImputer(strategy='mean')\n",
    "    imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "    imputed_X_valid = pd.DataFrame(my_imputer.transform(X_test))\n",
    "\n",
    "    # Imputation removed column names; put them back\n",
    "    imputed_X_train.columns = X_train.columns\n",
    "    imputed_X_valid.columns = X_test.columns\n",
    "    return imputed_X_train,imputed_X_valid\n",
    "\n",
    "def dropTypes(df:pd.DataFrame,exclude:str|list)->pd.DataFrame:\n",
    "    return df.select_dtypes(exclude=exclude)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `.fit()` || `.fit_transform()` || `.transform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.61237244,  0.61237244],\n",
       "        [ 0.61237244,  1.83711731]]),\n",
       " array([[ 0.        ,  0.        ],\n",
       "        [ 1.22474487,  1.22474487],\n",
       "        [-1.22474487, -1.22474487]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit_transform 方法通常用于学习特征工程或数据预处理中的某些参数，并且将这些参数应用于数据集以进行转换。\n",
    "# transform 方法通常用于将先前学到的参数应用于新的数据集。\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,OrdinalEncoder,OneHotEncoder\n",
    "\n",
    "def scaling(argScaler:str):\n",
    "    # 创建一个标准化的对象\n",
    "    scaler = eval(f\"{argScaler}()\")\n",
    "\n",
    "    # 对训练数据使用fit_transform\n",
    "    train_data = [[170, 70], [180, 80], [160, 60]]\n",
    "    scaled_train_data = scaler.fit_transform(train_data)\n",
    "\n",
    "    # 对测试数据使用transform\n",
    "    test_data = [[165, 75], [175, 85]]\n",
    "    scaled_test_data = scaler.transform(test_data)\n",
    "    return scaled_test_data,scaled_train_data\n",
    "\n",
    "scaling('StandardScaler')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropMissingCols(X:pd.DataFrame,Xtest:pd.DataFrame)->None:\n",
    "    cols_with_missing = [col for col in X.columns if X[col].isnull().any()] \n",
    "    X.drop(cols_with_missing, axis=1, inplace=True)\n",
    "    Xtest.drop(cols_with_missing, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterUsefulColsForOrdinalEncoder(X_train,X_valid):\n",
    "    # Categorical columns in the training data\n",
    "    object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "\n",
    "    # Columns that can be safely ordinal encoded\n",
    "    good_label_cols = [col for col in object_cols if \n",
    "                    set(X_valid[col]).issubset(set(X_train[col]))]\n",
    "            \n",
    "    # Problematic columns that will be dropped from the dataset\n",
    "    bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
    "            \n",
    "    print('Categorical columns that will be ordinal encoded:', good_label_cols)\n",
    "    print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)\n",
    "\n",
    "\n",
    "def applyOrdinalEncoder():\n",
    "# Apply ordinal encoder \n",
    "    oe=OrdinalEncoder() # Your code here\n",
    "    label_X_train=pd.DataFrame(oe.fit_transform(label_X_train)) # note that you should convert this into pd.DataFrame\n",
    "    label_X_valid=pd.DataFrame(oe.fit_transform(label_X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def howManyUniqueValues():\n",
    "    # count how many unique for each col\n",
    "    df = pd.DataFrame({'A': [4, 5, 6], 'B': [4, 1, 1]})\n",
    "    return df.nunique()\n",
    "\n",
    "def intermediateFuncs(X_train,object_cols):\n",
    "    # Get number of unique entries in each column with categorical data\n",
    "    object_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\n",
    "    d = dict(zip(object_cols, object_nunique))\n",
    "\n",
    "    # Print number of unique entries by column, in ascending order\n",
    "    '''按照元组第二个元素排序'''\n",
    "    sorted(d.items(), key=lambda x: x[1])\n",
    "    '''[('Street', 2),\n",
    " ('Utilities', 2),\n",
    " ('CentralAir', 2),\n",
    " ('LandSlope', 3),\n",
    " ('PavedDrive', 3),\n",
    " ('LotShape', 4),\n",
    " ('LandContour', 4),\n",
    " ('ExterQual', 4),\n",
    " ('KitchenQual', 4),\n",
    " ('MSZoning', 5),\n",
    " ('LotConfig', 5),\n",
    " ('BldgType', 5),\n",
    " ('ExterCond', 5),\n",
    " ('HeatingQC', 5),\n",
    " ('Condition2', 6),\n",
    " ('RoofStyle', 6),\n",
    " ('Foundation', 6),\n",
    " ('Heating', 6),\n",
    " ('Functional', 6),\n",
    " ('SaleCondition', 6),\n",
    " ('RoofMatl', 7),\n",
    " ('HouseStyle', 8),\n",
    " ('Condition1', 9),\n",
    " ('SaleType', 9),\n",
    " ('Exterior1st', 15),\n",
    " ('Exterior2nd', 16),\n",
    " ('Neighborhood', 25)]''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'Alice'), (1, 'Bob'), (2, 'Cindy')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names=['Alice','Bob','Cindy']\n",
    "list(enumerate(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 25, 49, 81, 100]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ints=[1,2,3,5,7,9,10]\n",
    "power=list(map(lambda i:i**2,ints))\n",
    "power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 10]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def isEven(x):\n",
    "    if x%2==0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "evenNums=list(filter(isEven,ints))\n",
    "evenNums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'admin'), (77, 'Fiona'), (777, 'Fynn'), (888, 'Finkenstr. 8')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complexList=[\n",
    "    (0,'admin'),\n",
    "    (777,'Fynn'),\n",
    "    (77,'Fiona'),\n",
    "    (888,'Finkenstr. 8')\n",
    "]\n",
    "sorted(complexList,key=lambda tup:tup[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the line below: How many categorical variables in the training data\n",
    "# have cardinality greater than 10?\n",
    "complexStruct=[('Street', 2),\n",
    " ('Utilities', 2),\n",
    " ('CentralAir', 2),\n",
    " ('LandSlope', 3),\n",
    " ('PavedDrive', 3),\n",
    " ('LotShape', 4),\n",
    " ('LandContour', 4),\n",
    " ('ExterQual', 4),\n",
    " ('KitchenQual', 4),\n",
    " ('MSZoning', 5),\n",
    " ('LotConfig', 5),\n",
    " ('BldgType', 5),\n",
    " ('ExterCond', 5),\n",
    " ('HeatingQC', 5),\n",
    " ('Condition2', 6),\n",
    " ('RoofStyle', 6),\n",
    " ('Foundation', 6),\n",
    " ('Heating', 6),\n",
    " ('Functional', 6),\n",
    " ('SaleCondition', 6),\n",
    " ('RoofMatl', 7),\n",
    " ('HouseStyle', 8),\n",
    " ('Condition1', 9),\n",
    " ('SaleType', 9),\n",
    " ('Exterior1st', 15),\n",
    " ('Exterior2nd', 16),\n",
    " ('Neighborhood', 25)]\n",
    "# Fill in the line below: How many categorical variables in the training data\n",
    "# have cardinality greater than 10?\n",
    "def isGreaterThan10(tup):\n",
    "    if tup[1]>10:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "high_cardinality_numcols = len(list(map(lambda tup:tup[0],list(filter(isGreaterThan10,complexStruct)))))\n",
    "\n",
    "\n",
    "def isNeighborhood(tup):\n",
    "    if tup[0]=='Neighborhood':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "# Fill in the line below: How many columns are needed to one-hot encode the \n",
    "# 'Neighborhood' variable in the training data?\n",
    "num_cols_neighborhood = list(filter(isNeighborhood,complexStruct))[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def applyOHE(X_train,X_valid):\n",
    "    # Apply one-hot encoder to each column with categorical data\n",
    "    # Columns that will be one-hot encoded\n",
    "    \n",
    "    # Categorical columns in the training data\n",
    "    object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "    \n",
    "    low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n",
    "\n",
    "    # Columns that will be dropped from the dataset\n",
    "    high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n",
    "\n",
    "    print('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\n",
    "    print('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)\n",
    "    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\n",
    "    OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n",
    "\n",
    "    # One-hot encoding removed index; put it back\n",
    "    OH_cols_train.index = X_train.index\n",
    "    OH_cols_valid.index = X_valid.index\n",
    "\n",
    "    # Remove categorical columns (will replace with one-hot encoding)\n",
    "    num_X_train = X_train.drop(object_cols, axis=1)\n",
    "    num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "    # Add one-hot encoded columns to numerical features\n",
    "    OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "    OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "    # Ensure all columns have string type\n",
    "    OH_X_train.columns = OH_X_train.columns.astype(str)\n",
    "    OH_X_valid.columns = OH_X_valid.columns.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `sklearn.pipeline.Pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good thing about pipeline: Automatically apply(fit) models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer  # handle missing data\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def preprocessingWithPipeline(numerical_cols, categorical_cols, X_train, y_train, X_valid, y_valid, n_estimators=100, random_state=42):\n",
    "    # Preprocessing for numerical data\n",
    "    # strategy: 'mean','median','most_frequent','constant'\n",
    "    numerical_transformer = SimpleImputer(strategy='constant', fill_value=None)\n",
    "    # stragegy=“constant” : replace missing values with fill_value. Can be used with strings or numeric data.\n",
    "\n",
    "    # Preprocessing for categorical data\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        # “most_frequent”: replace missing using the most frequent value along each column. Can be used with strings or numeric data. If there is more than one such value, only the smallest is returned.\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # Bundle preprocessing for numerical and categorical data\n",
    "    preprocessor = ColumnTransformer(  # applies transformer to given Columns\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ])\n",
    "\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators, random_state=random_state)\n",
    "\n",
    "    # Bundle preprocessing and modeling code in a pipeline\n",
    "    my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                  ('model', model)\n",
    "                                  ])\n",
    "\n",
    "    # Preprocessing of training data, fit model\n",
    "    my_pipeline.fit(X=X_train, y=y_train)\n",
    "\n",
    "\n",
    "\n",
    "    # Now we have the model, we just need to plug in Xvalid and yvalid\n",
    "\n",
    "    # Preprocessing of validation data, get predictions\n",
    "    preds = my_pipeline.predict(X=X_valid)\n",
    "\n",
    "    # Evaluate the model\n",
    "    score = mean_absolute_error(y_true=y_valid, y_pred=preds)\n",
    "    print('MAE:', score)\n",
    "    return (\n",
    "        preds,\n",
    "        score\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def standardTrainTestSplitter(Xtrain,Xtest,ytrain,ytest):\n",
    "    return train_test_split(Xtrain,Xtest,ytrain,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation: \n",
    "## `sklearn.model_selection.cross_val_score()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "\n",
    "\n",
    "def crossValidationScoreWithPipeLine(X, y, my_pipeline: Pipeline | None = None):\n",
    "    if type(my_pipeline) != type(None):\n",
    "        my_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()),\n",
    "                                      ('model', RandomForestRegressor(n_estimators=50,\n",
    "                                                                      random_state=0))\n",
    "                                      ])\n",
    "    scores = -1 * cross_val_score(my_pipeline, X, y,\n",
    "                                  cv=5,\n",
    "                                  scoring='neg_mean_absolute_error')\n",
    "\n",
    "    print(\"MAE scores:\\n\", scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
